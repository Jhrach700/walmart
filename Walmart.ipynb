{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration\n",
    "\n",
    "Before diving straight into anything, it is useful to look into the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>-0.375866</td>\n",
       "      <td>0.427942</td>\n",
       "      <td>-0.922338</td>\n",
       "      <td>0.210758</td>\n",
       "      <td>0.109015</td>\n",
       "      <td>0.621001</td>\n",
       "      <td>-0.444421</td>\n",
       "      <td>0.089970</td>\n",
       "      <td>-0.707711</td>\n",
       "      <td>0.473700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "      <td>0.047819</td>\n",
       "      <td>0.115627</td>\n",
       "      <td>-1.781739</td>\n",
       "      <td>-0.272785</td>\n",
       "      <td>0.392783</td>\n",
       "      <td>1.094168</td>\n",
       "      <td>-0.975254</td>\n",
       "      <td>-0.353424</td>\n",
       "      <td>0.145543</td>\n",
       "      <td>-0.064961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "      <td>0.372868</td>\n",
       "      <td>-0.263291</td>\n",
       "      <td>-1.120545</td>\n",
       "      <td>-0.773828</td>\n",
       "      <td>0.830072</td>\n",
       "      <td>-1.727836</td>\n",
       "      <td>1.323876</td>\n",
       "      <td>-1.587291</td>\n",
       "      <td>-0.024916</td>\n",
       "      <td>0.082491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>0.059598</td>\n",
       "      <td>0.270797</td>\n",
       "      <td>0.961795</td>\n",
       "      <td>-1.804197</td>\n",
       "      <td>2.931330</td>\n",
       "      <td>1.891656</td>\n",
       "      <td>0.094252</td>\n",
       "      <td>-0.873467</td>\n",
       "      <td>-1.217680</td>\n",
       "      <td>-1.848046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "      <td>0.616319</td>\n",
       "      <td>0.291275</td>\n",
       "      <td>-1.113519</td>\n",
       "      <td>0.626864</td>\n",
       "      <td>-0.287989</td>\n",
       "      <td>-0.842649</td>\n",
       "      <td>-0.947257</td>\n",
       "      <td>1.198215</td>\n",
       "      <td>0.972420</td>\n",
       "      <td>-1.054313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y x1        x2        x3        x4        x5        x6        x7        x8  \\\n",
       "0  0  a -0.375866  0.427942 -0.922338  0.210758  0.109015  0.621001 -0.444421   \n",
       "1  0  b  0.047819  0.115627 -1.781739 -0.272785  0.392783  1.094168 -0.975254   \n",
       "2  1  d  0.372868 -0.263291 -1.120545 -0.773828  0.830072 -1.727836  1.323876   \n",
       "3  0  c  0.059598  0.270797  0.961795 -1.804197  2.931330  1.891656  0.094252   \n",
       "4  1  d  0.616319  0.291275 -1.113519  0.626864 -0.287989 -0.842649 -0.947257   \n",
       "\n",
       "         x9       x10       x11  \n",
       "0  0.089970 -0.707711  0.473700  \n",
       "1 -0.353424  0.145543 -0.064961  \n",
       "2 -1.587291 -0.024916  0.082491  \n",
       "3 -0.873467 -1.217680 -1.848046  \n",
       "4  1.198215  0.972420 -1.054313  "
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"dataset.csv\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 1700\n",
      "Number of features: 11\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(data_df)\n",
    "n_features = len(data_df.columns) - 1\n",
    "n_classes = len(set(data_df['y']))\n",
    "\n",
    "print('Number of examples: {0}'.format(n_train))\n",
    "print('Number of features: {0}'.format(n_features))\n",
    "print('Number of classes: {0}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='y', ylabel='count'>"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot counts of each class\n",
    "sns.countplot(x = 'y', data=data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to take note:\n",
    "\n",
    "-The first feature is categorical. For simplicity sake, we will use a one-hot encoding where each category is represented with a binary vector \n",
    "\n",
    "-We don't have a testing dataset, so we will want to split our dataset into a training/validation/test set in which we use our training + validation set to train our model, and then we can evaluate the true performance of our final trained model on our untouched test set\n",
    "\n",
    "-There is somewhat of a class imbalance, which can be an issue with binary classification problems. Can use stratified k fold cross validation to combat this. While data is being split into folds, each fold will have the same proportion of observations with a given categorical value. Some classification algorithms are extremely sensitive to class ratio they are trained on, so stratifying the data can help achieve more consistent prediction performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding of x1 feature\n",
    "one_hot_columns = pd.get_dummies(data_df.x1)\n",
    "df_encoded = pd.concat([data_df, one_hot_columns], axis=1)\n",
    "del df_encoded['x1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.375866</td>\n",
       "      <td>0.427942</td>\n",
       "      <td>-0.922338</td>\n",
       "      <td>0.210758</td>\n",
       "      <td>0.109015</td>\n",
       "      <td>0.621001</td>\n",
       "      <td>-0.444421</td>\n",
       "      <td>0.089970</td>\n",
       "      <td>-0.707711</td>\n",
       "      <td>0.473700</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.047819</td>\n",
       "      <td>0.115627</td>\n",
       "      <td>-1.781739</td>\n",
       "      <td>-0.272785</td>\n",
       "      <td>0.392783</td>\n",
       "      <td>1.094168</td>\n",
       "      <td>-0.975254</td>\n",
       "      <td>-0.353424</td>\n",
       "      <td>0.145543</td>\n",
       "      <td>-0.064961</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.372868</td>\n",
       "      <td>-0.263291</td>\n",
       "      <td>-1.120545</td>\n",
       "      <td>-0.773828</td>\n",
       "      <td>0.830072</td>\n",
       "      <td>-1.727836</td>\n",
       "      <td>1.323876</td>\n",
       "      <td>-1.587291</td>\n",
       "      <td>-0.024916</td>\n",
       "      <td>0.082491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.059598</td>\n",
       "      <td>0.270797</td>\n",
       "      <td>0.961795</td>\n",
       "      <td>-1.804197</td>\n",
       "      <td>2.931330</td>\n",
       "      <td>1.891656</td>\n",
       "      <td>0.094252</td>\n",
       "      <td>-0.873467</td>\n",
       "      <td>-1.217680</td>\n",
       "      <td>-1.848046</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.616319</td>\n",
       "      <td>0.291275</td>\n",
       "      <td>-1.113519</td>\n",
       "      <td>0.626864</td>\n",
       "      <td>-0.287989</td>\n",
       "      <td>-0.842649</td>\n",
       "      <td>-0.947257</td>\n",
       "      <td>1.198215</td>\n",
       "      <td>0.972420</td>\n",
       "      <td>-1.054313</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y        x2        x3        x4        x5        x6        x7        x8  \\\n",
       "0  0 -0.375866  0.427942 -0.922338  0.210758  0.109015  0.621001 -0.444421   \n",
       "1  0  0.047819  0.115627 -1.781739 -0.272785  0.392783  1.094168 -0.975254   \n",
       "2  1  0.372868 -0.263291 -1.120545 -0.773828  0.830072 -1.727836  1.323876   \n",
       "3  0  0.059598  0.270797  0.961795 -1.804197  2.931330  1.891656  0.094252   \n",
       "4  1  0.616319  0.291275 -1.113519  0.626864 -0.287989 -0.842649 -0.947257   \n",
       "\n",
       "         x9       x10       x11  a  b  c  d  \n",
       "0  0.089970 -0.707711  0.473700  1  0  0  0  \n",
       "1 -0.353424  0.145543 -0.064961  0  1  0  0  \n",
       "2 -1.587291 -0.024916  0.082491  0  0  0  1  \n",
       "3 -0.873467 -1.217680 -1.848046  0  0  1  0  \n",
       "4  1.198215  0.972420 -1.054313  0  0  0  1  "
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Check for any non-numeric values (in case there's missing/weird data we need to worry about)\n",
    "nd = df_encoded.applymap(np.isreal)\n",
    "print(False in nd) #All values are numeric so don't have to worry about this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We split our dataset into training + test sets\n",
    "\n",
    "#train_ratio = 0.75\n",
    "#validation_ratio = 0.15\n",
    "#test_ratio = 0.10\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_encoded.iloc[:,1:], \n",
    "                                                    df_encoded.iloc[:,0], \n",
    "                                                    #test_size=1 - train_ratio, \n",
    "                                                    test_size=0.10, \n",
    "                                                    \n",
    "                                                    random_state=1,\n",
    "                                                    stratify=df_encoded['y']) #Note when we split, we also stratify this data\n",
    "y_train = y_train.to_numpy()\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_ratio/(test_ratio + validation_ratio), random_state=1,stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is ready to be fed into a classifier to build a model for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhrac\\Anaconda3\\envs\\walmart\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhrac\\Anaconda3\\envs\\walmart\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhrac\\Anaconda3\\envs\\walmart\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhrac\\Anaconda3\\envs\\walmart\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhrac\\Anaconda3\\envs\\walmart\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "metric_names = ['f1', 'roc_auc', 'average_precision', 'accuracy', 'precision', 'recall']\n",
    "scores_df = pd.DataFrame(index=metric_names, columns=['Random-CV', 'Stratified-CV']) # to store the scores\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "stratKfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(8,8,8), activation='relu', solver='adam', max_iter=500)\n",
    "\n",
    "#for metric in metric_names:\n",
    "#    score1 = cross_val_score(clf, X_train, y_train, scoring=metric, cv=kfold).mean()\n",
    "#    score2 = cross_val_score(clf, X_train, y_train, scoring=metric, cv=stratKfold).mean()\n",
    "#    scores_df.loc[metric] = [score1, score2]\n",
    "\n",
    "validation_scores = []\n",
    "training_scores = []\n",
    "i = 0\n",
    "for train_index, validation_index in stratKfold.split(X_train, y_train):\n",
    "    print(\"iteration:\",i)\n",
    "    X_tr, X_validation = X_train[train_index], X_train[validation_index]\n",
    "    y_tr, y_validation = y_train[train_index], y_train[validation_index]\n",
    "    model = clf.fit(X_tr,y_tr)\n",
    "    validation_scores.append(model.score(X_validation,y_validation))\n",
    "    training_scores.append(model.score(X_tr,y_tr))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation scores from each iteration: [0.954248366013072, 0.9607843137254902, 0.9934640522875817, 0.9575163398692811, 0.9084967320261438]\n",
      "Average Stratified K Fold score: 0.9549019607843139\n"
     ]
    }
   ],
   "source": [
    "print(\"validation scores from each iteration:\",validation_scores)\n",
    "print(\"Average Stratified K Fold score:\",np.mean(validation_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[91  9]\n",
      " [ 7 63]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy: 0.9058823529411765\n"
     ]
    }
   ],
   "source": [
    "print(\"testing accuracy:\",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  5,  14,  48,  57,  62,  73,  76,  88,  93, 125, 135, 136, 142,\n",
      "       144, 153, 169], dtype=int64),)\n",
      "Incorrectly predicted examples: [[ 4.47306452e-01 -1.91686902e-02  1.33566094e+00 -3.81592342e-01\n",
      "  -1.47152078e+00 -9.16582389e-01  3.70393078e-01 -1.17338286e+00\n",
      "   1.14566751e+00 -2.20551934e+00 -6.18153558e-01 -5.54700196e-01\n",
      "  -5.09175077e-01  1.59426054e+00]\n",
      " [-9.26597726e-02 -7.32653303e-01  6.32164053e-01  8.50198329e-01\n",
      "  -9.41596612e-01  3.93298156e-01  2.03946714e+00 -1.83445079e+00\n",
      "  -3.74638376e-02  1.46199778e+00 -6.18153558e-01 -5.54700196e-01\n",
      "  -5.09175077e-01  1.59426054e+00]\n",
      " [ 1.95623405e-03 -3.08140950e-01 -9.73095912e-01 -1.25844332e+00\n",
      "  -1.27578829e+00 -1.59134586e-01 -2.81989712e-01  1.85999482e+00\n",
      "   2.27402946e-01 -1.25068545e+00 -6.18153558e-01  1.80277564e+00\n",
      "  -5.09175077e-01 -6.27250048e-01]\n",
      " [ 3.80007074e-01  1.39562182e-02 -1.43426840e+00  7.63109834e-01\n",
      "  -1.51200801e+00  4.49528361e-01 -7.82872076e-01  6.44019322e-01\n",
      "   1.92461776e+00  1.07337154e+00  1.61772101e+00 -5.54700196e-01\n",
      "  -5.09175077e-01 -6.27250048e-01]\n",
      " [ 1.96315171e-01 -9.24064062e-02 -3.94192156e-02 -1.54067426e+00\n",
      "  -8.03733555e-01 -2.81636104e-01  1.03895269e+00 -5.86147391e-01\n",
      "  -8.82611959e-01 -8.79842074e-01 -6.18153558e-01 -5.54700196e-01\n",
      "   1.96396101e+00 -6.27250048e-01]\n",
      " [ 1.12437271e-01 -1.37299042e-01  1.41030283e+00  7.48594906e-02\n",
      "  -1.34010429e-01 -2.07129003e-01  6.23755204e-01 -1.07526478e-01\n",
      "  -2.37796203e+00  5.21732206e-01 -6.18153558e-01 -5.54700196e-01\n",
      "  -5.09175077e-01  1.59426054e+00]\n",
      " [ 3.05159156e-02 -2.24502556e-01 -1.63645520e+00 -8.00567794e-01\n",
      "  -2.06589191e-01  1.21350920e+00  1.14664563e+00  5.39621116e-02\n",
      "  -3.18523609e-01 -2.01196855e+00 -6.18153558e-01  1.80277564e+00\n",
      "  -5.09175077e-01 -6.27250048e-01]\n",
      " [ 6.95545659e-02 -1.45898535e-01 -6.52398565e-01 -4.78208418e-01\n",
      "  -7.92511946e-01  5.80763944e-01  1.49086617e+00 -8.37808462e-01\n",
      "   5.94407522e-01 -4.15672264e-01 -6.18153558e-01 -5.54700196e-01\n",
      "   1.96396101e+00 -6.27250048e-01]\n",
      " [ 1.41420442e+00  1.17930539e+00 -7.13130983e-01 -1.06024999e+00\n",
      "   1.23813509e+00 -1.25569602e+00  1.81650607e+00 -3.34700518e-01\n",
      "  -2.10562442e+00  3.02467225e-01  1.61772101e+00 -5.54700196e-01\n",
      "  -5.09175077e-01 -6.27250048e-01]\n",
      " [ 1.43488469e-01 -1.30030275e-01  3.86644852e-01  6.69911631e-01\n",
      "  -1.22928115e+00  1.83446988e+00  2.28194159e-01  4.68692077e-01\n",
      "  -2.22446844e+00  1.21502330e+00 -6.18153558e-01  1.80277564e+00\n",
      "  -5.09175077e-01 -6.27250048e-01]\n",
      " [ 4.36890658e-01  3.03341800e-01  4.15378790e-01 -9.25050005e-01\n",
      "  -1.50525924e+00  3.84593469e-01  4.33372459e-01  9.35734888e-01\n",
      "  -1.38712193e-01 -1.32992877e+00 -6.18153558e-01  1.80277564e+00\n",
      "  -5.09175077e-01 -6.27250048e-01]\n",
      " [ 2.73172173e-01 -2.20930987e-02  1.29285366e+00 -2.56722087e-01\n",
      "   8.87367202e-01  2.76479283e-02  4.24080000e-02 -2.45995045e-01\n",
      "  -5.90935206e-01 -1.67700784e+00 -6.18153558e-01 -5.54700196e-01\n",
      "  -5.09175077e-01  1.59426054e+00]\n",
      " [ 5.26817166e-02 -2.37511986e-01 -8.76053283e-01  1.61133913e-01\n",
      "  -6.83396887e-01  4.98505457e-01  1.78833156e-01  6.43992512e-01\n",
      "  -4.87486969e-01 -5.98848935e-01 -6.18153558e-01  1.80277564e+00\n",
      "  -5.09175077e-01 -6.27250048e-01]\n",
      " [ 7.77858275e-02 -1.41522548e-01 -8.80395853e-01 -1.60438011e+00\n",
      "   3.84983870e-01 -1.33656879e+00 -1.53836590e+00  1.28325640e+00\n",
      "  -2.61437449e-01  2.00372544e+00 -6.18153558e-01 -5.54700196e-01\n",
      "  -5.09175077e-01  1.59426054e+00]\n",
      " [ 3.72021354e-01 -1.24032075e-01  6.47052888e-01 -2.77552817e-01\n",
      "  -4.59014332e-03  7.49204354e-01  1.74771457e+00  5.12114809e-01\n",
      "  -1.03421958e-01  1.64149125e-01 -6.18153558e-01  1.80277564e+00\n",
      "  -5.09175077e-01 -6.27250048e-01]\n",
      " [ 1.83115452e-01 -1.48864291e-01 -1.32869940e-01 -4.26625991e-01\n",
      "   9.21281462e-01 -6.05601250e-02  5.91610465e-01 -6.34162003e-01\n",
      "   1.20004002e+00 -5.66435577e-01 -6.18153558e-01 -5.54700196e-01\n",
      "   1.96396101e+00 -6.27250048e-01]]\n",
      "---------------------------------------------------\n",
      "Correctly predicted examples [[-0.72687845  1.27077384  0.8546915  ... -0.5547002  -0.50917508\n",
      "   1.59426054]\n",
      " [ 1.26688858  0.00793772  0.59616885 ... -0.5547002  -0.50917508\n",
      "  -0.62725005]\n",
      " [ 0.30149209 -1.01389366 -0.30153968 ... -0.5547002  -0.50917508\n",
      "  -0.62725005]\n",
      " ...\n",
      " [ 0.55294485  0.33226362 -0.18068092 ... -0.5547002   1.96396101\n",
      "  -0.62725005]\n",
      " [ 0.94535846 -0.77321304  0.56054643 ... -0.5547002  -0.50917508\n",
      "  -0.62725005]\n",
      " [ 0.08268724 -0.87451567  1.22594997 ... -0.5547002   1.96396101\n",
      "  -0.62725005]]\n"
     ]
    }
   ],
   "source": [
    "diff = y_test.to_numpy() == y_pred\n",
    "#print(diff)\n",
    "ind_right = np.where(diff == True)\n",
    "ind_wrong = np.where(diff == False)\n",
    "print(ind_wrong)\n",
    "print(\"Incorrectly predicted examples:\",X_test[ind_wrong])\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Correctly predicted examples\",X_test[ind_right])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) I would say that even with a ~95% testing accuracy, I would say my model is extremely far from being the best model. That is because I only experimented with a neural network classifier and didn't do any hyperparameter tuning. Ideally, I would want to run experiments using many more machine learning classification algorithms and even run gridsearchcv's on each one in order to find the best possible hyperparameters. As far as an example of a data point that model doesn't do well, I found multiple examples above and it seemed pretty hard to tell without knowing the context of where the data came frome. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)\n",
    "\n",
    "1- I would probably just normalize this particular column in order to make all the elements lie in the range from 0 to 1.\n",
    "\n",
    "2- Ran out of time to actually implement, but I would try to run a few trials, starting with obtaining the median value of the column as the threshold, getting the accuracy, and comparing it with the accuracies of other trials such as the median of the 2nd half and first half of the data and taking the threshold that returns the maximum accuracy\n",
    "\n",
    "3- See refactored code below. By specifying a threshold dictionary, the user can enter the threshold values for each column they would like to change. For example, threshold_dic = {\"x2\":0,\"x5\":0,\"x11\":0} would mean that the x2, x5, and x11 features would be filtered by the threshold value of 0. Values greater than 0 are changed to 1, while values below are changed to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Refactoring code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename,threshold_dic={}):\n",
    "    data_df = pd.read_csv(filename)\n",
    "    one_hot_columns = pd.get_dummies(data_df.x1)\n",
    "    df_encoded = pd.concat([data_df, one_hot_columns], axis=1)\n",
    "    del df_encoded['x1']\n",
    "    #User can specify which columns to change based on theshold value\n",
    "    #threshold_dic = {\"x2\":0,\"x5\":0,\"x11\":0}\n",
    "    if (threshold_dic):\n",
    "        for k,v in threshold_dic.items():\n",
    "            df_encoded[k] >= v\n",
    "            df_encoded[k] = np.where(df[k] >= v, 1, 0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_encoded.iloc[:,1:], \n",
    "                                                    df_encoded.iloc[:,0], \n",
    "                                                    #test_size=1 - train_ratio, \n",
    "                                                    test_size=0.10, \n",
    "                                                    \n",
    "                                                    random_state=1,\n",
    "                                                    stratify=df_encoded['y']) #Note when we split, we also stratify this data\n",
    "    y_train = y_train.to_numpy()\n",
    "    #Standardizing the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train,X_test,y_train,y_test):\n",
    "    metric_names = ['f1', 'roc_auc', 'average_precision', 'accuracy', 'precision', 'recall']\n",
    "    scores_df = pd.DataFrame(index=metric_names, columns=['Random-CV', 'Stratified-CV']) # to store the scores\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    stratKfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(8,8,8), activation='relu', solver='adam', max_iter=500)\n",
    "\n",
    "#for metric in metric_names:\n",
    "#    score1 = cross_val_score(clf, X_train, y_train, scoring=metric, cv=kfold).mean()\n",
    "#    score2 = cross_val_score(clf, X_train, y_train, scoring=metric, cv=stratKfold).mean()\n",
    "#    scores_df.loc[metric] = [score1, score2]\n",
    "\n",
    "    validation_scores = []\n",
    "    training_scores = []\n",
    "    i = 0\n",
    "    for train_index, validation_index in stratKfold.split(X_train, y_train):\n",
    "        print(\"iteration:\",i)\n",
    "        X_tr, X_validation = X_train[train_index], X_train[validation_index]\n",
    "        y_tr, y_validation = y_train[train_index], y_train[validation_index]\n",
    "        model = clf.fit(X_tr,y_tr)\n",
    "        validation_scores.append(model.score(X_validation,y_validation))\n",
    "        training_scores.append(model.score(X_tr,y_tr))\n",
    "        i += 1\n",
    "    return model,validation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(model,validation_scores):\n",
    "    print(\"validation scores from each iteration:\",validation_scores)\n",
    "    print(\"Average Stratified K Fold score:\",np.mean(validation_scores))\n",
    "    print(\"------\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"confusion matrix:\")\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"------\")\n",
    "    print(\"testing accuracy:\",accuracy_score(y_test,y_pred))\n",
    "    print(\"------\")\n",
    "    diff = y_test.to_numpy() == y_pred\n",
    "    #print(diff)\n",
    "    ind_right = np.where(diff == True)\n",
    "    ind_wrong = np.where(diff == False)\n",
    "    print(ind_wrong)\n",
    "    print(\"Incorrectly predicted examples:\",X_test[ind_wrong])\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"Correctly predicted examples\",X_test[ind_right])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhrac\\Anaconda3\\envs\\walmart\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1\n",
      "iteration: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhrac\\Anaconda3\\envs\\walmart\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhrac\\Anaconda3\\envs\\walmart\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhrac\\Anaconda3\\envs\\walmart\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = preprocess(\"dataset.csv\",threshold_dic = {\"x2\":0,\"x5\":0,\"x11\":0})\n",
    "model,validation_scores = train(X_train,X_test,y_train,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation scores from each iteration: [0.8496732026143791, 0.8366013071895425, 0.8366013071895425, 0.8137254901960784, 0.8366013071895425]\n",
      "Average Stratified K Fold score: 0.8346405228758169\n",
      "------\n",
      "confusion matrix:\n",
      "[[89 11]\n",
      " [18 52]]\n",
      "------\n",
      "testing accuracy: 0.8294117647058824\n",
      "------\n",
      "(array([  5,   9,  14,  19,  20,  31,  33,  39,  42,  49,  52,  57,  62,\n",
      "        76,  82,  88,  93,  98, 100, 106, 113, 119, 120, 121, 140, 146,\n",
      "       153, 157, 169], dtype=int64),)\n",
      "Incorrectly predicted examples: [[ 0.88852332 -0.01916869  1.33566094 -1.03593954 -1.47152078 -0.91658239\n",
      "   0.37039308 -1.17338286  1.14566751 -1.03593954 -0.61815356 -0.5547002\n",
      "  -0.50917508  1.59426054]\n",
      " [-1.12546287 -0.84327049  1.1304256   0.9653073   0.00807053  0.04432526\n",
      "   2.82625442 -0.37132683  0.63256409 -1.03593954 -0.61815356 -0.5547002\n",
      "  -0.50917508  1.59426054]\n",
      " [-1.12546287 -0.7326533   0.63216405  0.9653073  -0.94159661  0.39329816\n",
      "   2.03946714 -1.83445079 -0.03746384  0.9653073  -0.61815356 -0.5547002\n",
      "  -0.50917508  1.59426054]\n",
      " [-1.12546287 -0.45761668  0.76366856  0.9653073  -0.75174979 -0.23227032\n",
      "  -0.82474821 -1.7390066  -0.64461907 -1.03593954 -0.61815356  1.80277564\n",
      "  -0.50917508 -0.62725005]\n",
      " [ 0.88852332 -0.05830593 -1.35172581  0.9653073  -0.32479762  0.71850707\n",
      "  -0.97686733  0.32327803 -1.5188925   0.9653073  -0.61815356 -0.5547002\n",
      "  -0.50917508  1.59426054]\n",
      " [ 0.88852332 -0.21803109 -2.24649036  0.9653073   1.09316168 -0.52072667\n",
      "   0.59551086  0.95526967  0.13588767 -1.03593954  1.61772101 -0.5547002\n",
      "  -0.50917508 -0.62725005]\n",
      " [-1.12546287 -0.94231581  1.36659306  0.9653073  -1.26532222 -1.34454873\n",
      "  -0.77947719  0.78821839  0.0687387  -1.03593954 -0.61815356 -0.5547002\n",
      "  -0.50917508  1.59426054]\n",
      " [ 0.88852332  0.91329614  0.07674226 -1.03593954 -0.33718301  0.60869583\n",
      "   1.28691103  0.61874322 -0.02182925 -1.03593954 -0.61815356 -0.5547002\n",
      "   1.96396101 -0.62725005]\n",
      " [ 0.88852332 -0.12135138  0.44680283  0.9653073   0.88688872 -2.32161453\n",
      "   1.06627488 -0.48415524  0.64953143  0.9653073  -0.61815356  1.80277564\n",
      "  -0.50917508 -0.62725005]\n",
      " [ 0.88852332  0.24444745 -0.77110625 -1.03593954 -1.39250235 -0.80585823\n",
      "  -0.55651096  0.51642809 -1.95975751 -1.03593954 -0.61815356 -0.5547002\n",
      "  -0.50917508  1.59426054]\n",
      " [ 0.88852332  0.56615077  0.77392049 -1.03593954  0.00902402 -1.75410619\n",
      "   0.34371459 -0.25444028 -0.37004602  0.9653073  -0.61815356 -0.5547002\n",
      "   1.96396101 -0.62725005]\n",
      " [ 0.88852332  0.01395622 -1.4342684   0.9653073  -1.51200801  0.44952836\n",
      "  -0.78287208  0.64401932  1.92461776  0.9653073   1.61772101 -0.5547002\n",
      "  -0.50917508 -0.62725005]\n",
      " [ 0.88852332 -0.09240641 -0.03941922 -1.03593954 -0.80373356 -0.2816361\n",
      "   1.03895269 -0.58614739 -0.88261196 -1.03593954 -0.61815356 -0.5547002\n",
      "   1.96396101 -0.62725005]\n",
      " [-1.12546287 -0.22450256 -1.6364552  -1.03593954 -0.20658919  1.2135092\n",
      "   1.14664563  0.05396211 -0.31852361 -1.03593954 -0.61815356  1.80277564\n",
      "  -0.50917508 -0.62725005]\n",
      " [ 0.88852332  0.78877244  1.51906659 -1.03593954 -0.95217767 -1.17081582\n",
      "  -0.61205533 -0.56869069 -0.38044527  0.9653073  -0.61815356 -0.5547002\n",
      "  -0.50917508  1.59426054]\n",
      " [-1.12546287 -0.14589853 -0.65239856 -1.03593954 -0.79251195  0.58076394\n",
      "   1.49086617 -0.83780846  0.59440752 -1.03593954 -0.61815356 -0.5547002\n",
      "   1.96396101 -0.62725005]\n",
      " [ 0.88852332  1.17930539 -0.71313098 -1.03593954  1.23813509 -1.25569602\n",
      "   1.81650607 -0.33470052 -2.10562442  0.9653073   1.61772101 -0.5547002\n",
      "  -0.50917508 -0.62725005]\n",
      " [-1.12546287 -0.96229955  1.38988915  0.9653073   0.72462709  0.45378994\n",
      "   0.04830204  0.01478448 -1.13992123 -1.03593954  1.61772101 -0.5547002\n",
      "  -0.50917508 -0.62725005]\n",
      " [ 0.88852332  0.93655222  0.88699647  0.9653073  -1.48776379 -1.44603843\n",
      "   0.88133156 -0.88275662  1.60803766  0.9653073  -0.61815356 -0.5547002\n",
      "  -0.50917508  1.59426054]\n",
      " [ 0.88852332  0.47043858  0.54864826 -1.03593954 -1.85499933 -0.86852848\n",
      "   0.36011196 -0.29818997  2.11192942 -1.03593954 -0.61815356 -0.5547002\n",
      "   1.96396101 -0.62725005]\n",
      " [ 0.88852332  0.84850162  0.72778071 -1.03593954 -0.66945097  1.58738151\n",
      "  -0.56128583 -1.14731326 -1.30515267  0.9653073  -0.61815356  1.80277564\n",
      "  -0.50917508 -0.62725005]\n",
      " [-1.12546287 -0.18999377  1.11725271 -1.03593954  0.60103762  0.37124623\n",
      "  -0.52882978  0.70774247  1.41272042 -1.03593954 -0.61815356 -0.5547002\n",
      "  -0.50917508  1.59426054]\n",
      " [-1.12546287 -0.82504733 -1.46887114  0.9653073   0.92434457 -1.28265297\n",
      "   0.05780412 -1.65132881  1.04618996  0.9653073  -0.61815356 -0.5547002\n",
      "  -0.50917508  1.59426054]\n",
      " [-1.12546287 -0.18171245  1.05839576 -1.03593954  0.1470068   0.15430687\n",
      "  -0.50439309 -1.32418018  0.72549441  0.9653073   1.61772101 -0.5547002\n",
      "  -0.50917508 -0.62725005]\n",
      " [ 0.88852332 -0.12439288  0.87982172  0.9653073   0.41201357  0.38628635\n",
      "   0.87841941 -0.31463361 -0.35042892 -1.03593954  1.61772101 -0.5547002\n",
      "  -0.50917508 -0.62725005]\n",
      " [-1.12546287 -0.44733204  0.32869933 -1.03593954  0.88739632 -1.64102278\n",
      "  -0.45496413  0.63443065 -1.38691409 -1.03593954 -0.61815356 -0.5547002\n",
      "   1.96396101 -0.62725005]\n",
      " [ 0.88852332 -0.12403208  0.64705289 -1.03593954 -0.00459014  0.74920435\n",
      "   1.74771457  0.51211481 -0.10342196  0.9653073  -0.61815356  1.80277564\n",
      "  -0.50917508 -0.62725005]\n",
      " [ 0.88852332  0.10445452  0.5113738   0.9653073   0.21821736  1.46864827\n",
      "   1.09269499  0.99661483  0.37118005  0.9653073  -0.61815356 -0.5547002\n",
      "  -0.50917508  1.59426054]\n",
      " [ 0.88852332 -0.14886429 -0.13286994 -1.03593954  0.92128146 -0.06056012\n",
      "   0.59161047 -0.634162    1.20004002 -1.03593954 -0.61815356 -0.5547002\n",
      "   1.96396101 -0.62725005]]\n",
      "---------------------------------------------------\n",
      "Correctly predicted examples [[-1.12546287  1.27077384  0.8546915  ... -0.5547002  -0.50917508\n",
      "   1.59426054]\n",
      " [ 0.88852332  0.00793772  0.59616885 ... -0.5547002  -0.50917508\n",
      "  -0.62725005]\n",
      " [ 0.88852332 -1.01389366 -0.30153968 ... -0.5547002  -0.50917508\n",
      "  -0.62725005]\n",
      " ...\n",
      " [ 0.88852332  0.33226362 -0.18068092 ... -0.5547002   1.96396101\n",
      "  -0.62725005]\n",
      " [ 0.88852332 -0.77321304  0.56054643 ... -0.5547002  -0.50917508\n",
      "  -0.62725005]\n",
      " [ 0.88852332 -0.87451567  1.22594997 ... -0.5547002   1.96396101\n",
      "  -0.62725005]]\n"
     ]
    }
   ],
   "source": [
    "display_results(model,validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
